#!/usr/bin/env python3
"""
ç¬¬ä¸€æ­¥ï¼šåµæ¸¬ä¸¦æ¸…ç†é‡è¤‡åœ–ç‰‡
ä½¿ç”¨æ–‡ä»¶å…§å®¹å“ˆå¸Œç¢ºä¿æº–ç¢ºçš„é‡è¤‡åµæ¸¬
"""

import hashlib
import json
from pathlib import Path
from collections import defaultdict

downloads_dir = Path("/Users/hsiaojohnny/Downloads")
session_dir = Path("/Users/hsiaojohnny/.copilot/session-state/0627c76d-21e0-4128-b7ff-ea283b16e7d2")

print("ğŸ” ç¬¬ä¸€æ­¥ï¼šåµæ¸¬é‡è¤‡åœ–ç‰‡æª”æ¡ˆ")
print("=" * 70)
print()

# æƒææ‰€æœ‰åœ–ç‰‡
image_files = []
for file_path in sorted(downloads_dir.glob("*")):
    if file_path.is_file() and file_path.suffix.lower() in {'.png', '.jpg', '.jpeg', '.webp', '.gif'}:
        image_files.append(file_path)

print(f"ğŸ“‹ æƒæå®Œæˆï¼š{len(image_files)} å€‹åœ–ç‰‡æª”æ¡ˆ")
print()

# è¨ˆç®—æª”æ¡ˆå“ˆå¸Œå€¼
print("ğŸ” è¨ˆç®—æª”æ¡ˆå“ˆå¸Œå€¼...")
file_hashes = defaultdict(list)

for file_path in image_files:
    try:
        # è¨ˆç®—æ–‡ä»¶å…§å®¹çš„ MD5 å“ˆå¸Œ
        md5_hash = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                md5_hash.update(chunk)
        
        file_hash = md5_hash.hexdigest()
        file_hashes[file_hash].append(file_path)
    except:
        pass

print(f"âœ… å“ˆå¸Œè¨ˆç®—å®Œæˆ")
print()

# æ‰¾å‡ºé‡è¤‡æª”æ¡ˆ
duplicates_to_delete = []
duplicate_info = []

for file_hash, files in file_hashes.items():
    if len(files) > 1:
        # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œä¿ç•™æœ€æ–°çš„ï¼Œåˆªé™¤èˆŠçš„
        sorted_files = sorted(files, key=lambda x: x.stat().st_mtime, reverse=True)
        
        # ä¿ç•™ç¬¬ä¸€å€‹ï¼ˆæœ€æ–°çš„ï¼‰ï¼Œå…¶ä»–æ¨™è¨˜ç‚ºé‡è¤‡
        for dup_file in sorted_files[1:]:
            duplicates_to_delete.append(dup_file)
            duplicate_info.append({
                "keep": sorted_files[0].name,
                "delete": dup_file.name,
                "hash": file_hash,
                "size": dup_file.stat().st_size
            })

print(f"ğŸ“Š é‡è¤‡åµæ¸¬çµæœ:")
print(f"  ç¸½è¨ˆæª”æ¡ˆï¼š{len(image_files)}")
print(f"  å”¯ä¸€æª”æ¡ˆï¼š{len(image_files) - len(duplicates_to_delete)}")
print(f"  é‡è¤‡å‰¯æœ¬ï¼š{len(duplicates_to_delete)}")
print()

if duplicates_to_delete:
    print("ğŸ—‘ï¸ è¦åˆªé™¤çš„é‡è¤‡æª”æ¡ˆæ¸…å–®ï¼š")
    print()
    for info in duplicate_info[:10]:  # é¡¯ç¤ºå‰10å€‹
        print(f"  ä¿ç•™: {info['keep']}")
        print(f"  åˆªé™¤: {info['delete']}")
        print()
    
    if len(duplicate_info) > 10:
        print(f"  ... é‚„æœ‰ {len(duplicate_info) - 10} å€‹é‡è¤‡")
    
    print()
    print("é–‹å§‹åˆªé™¤é‡è¤‡æª”æ¡ˆ...")
    
    deleted_count = 0
    for dup_file in duplicates_to_delete:
        try:
            dup_file.unlink()
            deleted_count += 1
            print(f"  âœ… å·²åˆªé™¤: {dup_file.name}")
        except Exception as e:
            print(f"  âŒ åˆªé™¤å¤±æ•—: {dup_file.name} - {e}")
    
    print()
    print(f"âœ… æˆåŠŸåˆªé™¤ï¼š{deleted_count} å€‹é‡è¤‡æª”æ¡ˆ")
else:
    print("âœ… æ²’æœ‰ç™¼ç¾é‡è¤‡æª”æ¡ˆ")

# ä¿å­˜æ¸…ç†å ±å‘Š
cleanup_report = {
    "original_count": len(image_files),
    "duplicates_found": len(duplicates_to_delete),
    "remaining_count": len(image_files) - len(duplicates_to_delete),
    "duplicate_details": duplicate_info
}

with open(session_dir / "cleanup_report.json", "w", encoding="utf-8") as f:
    json.dump(cleanup_report, f, ensure_ascii=False, indent=2)

print()
print("=" * 70)
print(f"âœ… æ¸…ç†å®Œæˆ")
print(f"ğŸ“Š æœ€çµ‚çµæœï¼š")
print(f"   åŸå§‹æª”æ¡ˆæ•¸ï¼š{cleanup_report['original_count']}")
print(f"   å·²åˆªé™¤ï¼š{cleanup_report['duplicates_found']}")
print(f"   ä¿ç•™æª”æ¡ˆæ•¸ï¼š{cleanup_report['remaining_count']}")
