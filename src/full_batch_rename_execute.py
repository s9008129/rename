#!/usr/bin/env python3
"""
å®Œæ•´åŸ·è¡Œï¼šä½¿ç”¨ Qwen3-VL é€²è¡Œå…¨é‡åœ–ç‰‡åˆ†æå’Œç²¾æº–é‡å‘½å

æµç¨‹ï¼š
1. åˆ†æå…¨éƒ¨åœ–ç‰‡
2. ç”Ÿæˆç²¾æº–å‘½åå°ç…§è¡¨
3. åŸ·è¡Œæª”æ¡ˆé‡å‘½å
4. ç”Ÿæˆè©³ç´°å ±å‘Š

æ–°å¢åŠŸèƒ½ï¼ˆv1.1ï¼‰ï¼š
- å¢é‡æ¨¡å¼ï¼ˆé»˜èªï¼‰ï¼šè·³éå·²å‘½åçš„æª”æ¡ˆï¼ˆæª”ååŒ…å«ä¸­æ–‡ï¼‰
- å¼·åˆ¶é‡æ–°å‘½åæ¨¡å¼ï¼šé‡æ–°åˆ†æå’Œå‘½åæ‰€æœ‰æª”æ¡ˆ
- å…¨å±€æª”æ¡ˆè¿½è¹¤æ©Ÿåˆ¶
"""

import os
import json
import base64
import requests
from pathlib import Path
from typing import Dict, List, Optional
import time
from datetime import datetime
import argparse
import sys

# å°å…¥é€²åº¦è¿½è¹¤å™¨
from progress_tracker import ProgressTracker

# é…ç½®
# ä½¿ç”¨ç›¸å°è·¯å¾‘ï¼šPROJECT_ROOT æ‡‰è©²æ˜¯åŸ·è¡Œè…³æœ¬çš„ç›®éŒ„
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
LOGS_DIR = PROJECT_ROOT / "logs"
SESSION_DIR = DATA_DIR / "session"

LM_STUDIO_API = "http://127.0.0.1:1234/v1/chat/completions"
BATCH_SIZE = 10  # æ¯æ‰¹ 10 å¼µåœ–ç‰‡

# ç¢ºä¿å¿…è¦çš„ç›®éŒ„å­˜åœ¨
DATA_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)
SESSION_DIR.mkdir(parents=True, exist_ok=True)

# è§£æå‘½ä»¤è¡Œåƒæ•¸
parser = argparse.ArgumentParser(
    description="åœ–ç‰‡æ™ºèƒ½å‘½åç³»çµ± - ä½¿ç”¨ Qwen3-VL é€²è¡Œè¦–è¦ºåˆ†æå’Œé‡å‘½å"
)
parser.add_argument(
    "--force-rename",
    "--override",
    dest="force_rename",
    action="store_true",
    help="å¼·åˆ¶é‡æ–°å‘½åå·²å‘½åçš„æª”æ¡ˆï¼ˆå¢é‡æ¨¡å¼ï¼‰"
)
parser.add_argument(
    "--target-dir",
    default=None,
    help="æŒ‡å®šè¦è™•ç†çš„ç›®éŒ„ï¼ˆé»˜èªï¼šä½¿ç”¨äº¤äº’å¼æç¤ºè¼¸å…¥ï¼‰"
)
parser.add_argument(
    "--limit",
    type=int,
    default=None,
    help="é™åˆ¶è™•ç†çš„åœ–ç‰‡æ•¸é‡ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼Œé»˜èªï¼šç„¡é™åˆ¶ï¼‰"
)
parser.add_argument(
    "--delete-original",
    action="store_true",
    help="é‡å‘½åå¾Œåˆªé™¤åŸæª”æ¡ˆ"
)
args = parser.parse_args()

FORCE_RENAME = args.force_rename
LIMIT_IMAGES = args.limit  # æ–°å¢ï¼šé™åˆ¶åœ–ç‰‡æ•¸é‡
DELETE_ORIGINAL = args.delete_original  # æ–°å¢ï¼šæ˜¯å¦åˆªé™¤åŸæª”æ¡ˆ

# å¦‚æœæ²’æœ‰æŒ‡å®šç›®éŒ„ï¼Œä½¿ç”¨äº¤äº’å¼è¼¸å…¥æˆ–ç•¶å‰ç›®éŒ„
if args.target_dir:
    TARGET_DIR = Path(args.target_dir).expanduser()
else:
    # é»˜èªç‚ºç•¶å‰å·¥ä½œç›®éŒ„
    TARGET_DIR = Path.cwd()

print("=" * 80)
print("ğŸš€ åœ–ç‰‡æ™ºèƒ½å‘½åç³»çµ± - Qwen3-VL æ‰¹é‡åˆ†æå’Œé‡å‘½å v1.2")
print("=" * 80)
print(f"æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ç›®æ¨™ç›®éŒ„ï¼š{TARGET_DIR}")
if FORCE_RENAME:
    print("ğŸ“Œ æ¨¡å¼ï¼šå¼·åˆ¶é‡æ–°å‘½åï¼ˆå°‡é‡æ–°åˆ†ææ‰€æœ‰æª”æ¡ˆï¼‰")
else:
    print("ğŸ“Œ æ¨¡å¼ï¼šå¢é‡æ¨¡å¼ï¼ˆå°‡è·³éå·²å‘½åçš„æª”æ¡ˆï¼‰")
print()

# åˆå§‹åŒ–é€²åº¦è¿½è¹¤å™¨
progress = ProgressTracker(SESSION_DIR, "rename")

def is_already_renamed(filename: str) -> bool:
    """æª¢æ¸¬æª”æ¡ˆæ˜¯å¦å·²è¢«å‘½åï¼ˆæª”ååŒ…å«ä¸­æ–‡å­—ç¬¦ï¼‰"""
    import re
    return bool(re.search(r'[\u4e00-\u9fff]', filename))

# æƒææ‰€æœ‰åœ–ç‰‡ï¼ˆéè¿´æƒææ‰€æœ‰å­è³‡æ–™å¤¾ï¼‰
image_files = sorted([
    f for f in TARGET_DIR.rglob("*") 
    if f.is_file() and f.suffix.lower() in {'.png', '.jpg', '.jpeg', '.webp', '.gif', '.bmp'}
])

# æ‡‰ç”¨é™åˆ¶ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
if LIMIT_IMAGES:
    image_files = image_files[:LIMIT_IMAGES]

print(f"ğŸ“Š æƒæçµæœï¼šæ‰¾åˆ° {len(image_files)} å€‹åœ–ç‰‡æª”æ¡ˆ", end="")
if LIMIT_IMAGES:
    print(f"ï¼ˆå·²é™åˆ¶ç‚º {LIMIT_IMAGES} å¼µç”¨æ–¼æ¸¬è©¦ï¼‰")
else:
    print()

# æª¢æ¸¬å·²å‘½åå’Œæœªå‘½åçš„æª”æ¡ˆ
if not FORCE_RENAME:
    renamed_files = [f for f in image_files if is_already_renamed(f.stem)]
    unnamed_files = [f for f in image_files if not is_already_renamed(f.stem)]
    
    print(f"   å·²å‘½åï¼š{len(renamed_files)} å€‹")
    print(f"   æœªå‘½åï¼š{len(unnamed_files)} å€‹")
    
    if renamed_files:
        print(f"   ğŸ’¡ æç¤ºï¼šå·²å‘½åçš„æª”æ¡ˆå°‡è¢«è·³éã€‚ä½¿ç”¨ --force-rename é‡æ–°åˆ†ææ‰€æœ‰æª”æ¡ˆ")
    
    # å¢é‡æ¨¡å¼ï¼šåªè™•ç†æœªå‘½åçš„æª”æ¡ˆ
    image_files = unnamed_files
    print()
    print(f"âš™ï¸  é–‹å§‹è™•ç† {len(image_files)} å€‹æœªå‘½åçš„æª”æ¡ˆ...")
else:
    print(f"   æ‰¹æ¬¡å¤§å°ï¼š{BATCH_SIZE} å¼µ/æ‰¹")
    print(f"   é è¨ˆæ‰¹æ¬¡æ•¸ï¼š{(len(image_files) + BATCH_SIZE - 1) // BATCH_SIZE}")
    print()

print()

# åˆ†æçµæœå„²å­˜
analysis_results = []
failed_files = []
skipped_duplicates = []

def encode_image_to_base64(image_path: Path) -> str:
    """å°‡åœ–ç‰‡ç·¨ç¢¼ç‚º base64"""
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

def get_image_media_type(image_path: Path) -> str:
    """æ ¹æ“šå‰¯æª”åç¢ºå®š MIME é¡å‹"""
    ext = image_path.suffix.lower()
    return {
        '.png': 'image/png',
        '.jpg': 'image/jpeg',
        '.jpeg': 'image/jpeg',
        '.webp': 'image/webp',
        '.gif': 'image/gif'
    }.get(ext, 'image/png')

def analyze_image_with_qwen(image_path: Path, retry_count: int = 3) -> Dict:
    """ä½¿ç”¨ Qwen3-VL åˆ†æå–®å¼µåœ–ç‰‡ï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰"""
    
    for attempt in range(retry_count):
        try:
            # ç·¨ç¢¼åœ–ç‰‡
            image_base64 = encode_image_to_base64(image_path)
            media_type = get_image_media_type(image_path)
            
            # æº–å‚™åˆ†ææç¤º
            analysis_prompt = """è«‹æ·±åº¦åˆ†æé€™å¼µåœ–ç‰‡ä¸¦ç”¨å°ç£ç¹é«”ä¸­æ–‡å›ç­”ã€‚è¿”å› JSON æ ¼å¼çš„çµæœï¼ˆåªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š

{
  "image_title": "åœ–ç‰‡ä¸­çš„æ¨™é¡Œæ–‡å­—ï¼ˆå¦‚ç„¡æ¨™é¡Œå‰‡ç‚º 'N/A'ï¼‰",
  "main_theme": "æ ¸å¿ƒä¸»é¡Œåˆ†é¡ï¼ˆå¦‚ï¼šè²¡ç¶“ã€æŠ€è¡“ã€è¨­è¨ˆã€å ±å‘Šç­‰ï¼‰",
  "sub_theme": "å­åˆ†é¡ï¼ˆå¦‚ï¼šæŠ•è³‡åˆ†æã€AIç³»çµ±ã€å‰µæ„è¨­è¨ˆç­‰ï¼‰",
  "core_content": "åœ–ç‰‡çš„å…·é«”æ ¸å¿ƒå…§å®¹ï¼ˆé—œéµè©æˆ–çŸ­å¥ï¼Œ20å­—ä»¥å…§ï¼‰",
  "recommended_name": "æ¨è–¦å‘½åï¼ˆæ ¼å¼ï¼šä¸»é¡Œ_å­ä¸»é¡Œ_å…·é«”æ¨™é¡Œï¼Œæœ€å¤š25å­—ï¼Œä¸å«æ—¥æœŸï¼‰"
}"""
            
            # èª¿ç”¨ LM Studio API
            headers = {"Content-Type": "application/json"}
            
            payload = {
                "model": "qwen/qwen3-vl-30b",
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{media_type};base64,{image_base64}"
                                }
                            },
                            {
                                "type": "text",
                                "text": analysis_prompt
                            }
                        ]
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 500
            }
            
            response = requests.post(LM_STUDIO_API, json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            
            # è§£æå›æ‡‰
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            
            # æå– JSON
            try:
                analysis_json = json.loads(analysis_text)
            except json.JSONDecodeError:
                import re
                json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
                if json_match:
                    analysis_json = json.loads(json_match.group())
                else:
                    raise ValueError(f"ç„¡æ³•è§£æå›æ‡‰")
            
            return {
                "filename": str(image_path.relative_to(TARGET_DIR)),
                "status": "success",
                "analysis": analysis_json
            }
        
        except Exception as e:
            if attempt < retry_count - 1:
                time.sleep(2)  # é‡è©¦å‰ç­‰å¾…
                continue
            else:
                return {
                    "filename": str(image_path.relative_to(TARGET_DIR)),
                    "status": "error",
                    "error": str(e)
                }

# åŠ è¼‰ä¹‹å‰çš„çµæœï¼ˆå¦‚æœæœ‰ï¼‰
previous_results_file = SESSION_DIR / "qwen_vision_analysis_sample.json"
if previous_results_file.exists():
    print("ğŸ“‚ åŠ è¼‰ä¹‹å‰çš„æ¨£æœ¬åˆ†æçµæœ...")
    with open(previous_results_file, 'r', encoding='utf-8') as f:
        previous = json.load(f)
        analysis_results = previous.get('detailed_results', [])
    print(f"   å·²åŠ è¼‰ {len(analysis_results)} å€‹çµæœ")
    processed_files = {r['filename'] for r in analysis_results}
    remaining_files = [f for f in image_files if f.name not in processed_files]
    print(f"   å‰©é¤˜å¾…åˆ†æï¼š{len(remaining_files)} å¼µ")
    print()
else:
    remaining_files = image_files
    processed_files = set()

# æ‰¹é‡è™•ç†åœ–ç‰‡
print("ğŸš€ é–‹å§‹å…¨é‡åˆ†æ...")
print()

total_processed = len(analysis_results)
successful = sum(1 for r in analysis_results if r['status'] == 'success')
failed = sum(1 for r in analysis_results if r['status'] != 'success')

for batch_idx in range((len(remaining_files) + BATCH_SIZE - 1) // BATCH_SIZE):
    start_idx = batch_idx * BATCH_SIZE
    end_idx = min(start_idx + BATCH_SIZE, len(remaining_files))
    
    batch_files = remaining_files[start_idx:end_idx]
    batch_num = len(analysis_results) // BATCH_SIZE + batch_idx + 1
    
    # æ›´æ–°é€²åº¦è¿½è¹¤
    progress.update_analysis(batch_num, BATCH_SIZE, total_processed)
    
    for img_idx, img_file in enumerate(batch_files, 1):
        print(f"   [{img_idx}/{len(batch_files)}] {img_file.name[:45]}... ", end="", flush=True)
        
        result = analyze_image_with_qwen(img_file)
        analysis_results.append(result)
        total_processed += 1
        
        if result['status'] == 'success':
            successful += 1
            print(f"âœ…")
        else:
            failed += 1
            failed_files.append(result)
            print(f"âŒ")
        
        # è¨ˆç®—ä¸¦è¼¸å‡ºé€²åº¦ç™¾åˆ†æ¯”
        progress_pct = int(total_processed * 100 / len(remaining_files)) if remaining_files else 0
        eta = progress.get_eta_seconds()
        eta_str = progress._format_time(eta) if eta > 0 else "è¨ˆç®—ä¸­..."
        print(f"[é€²åº¦] åˆ†æ: {progress_pct}% | {total_processed}/{len(remaining_files)} | ETA: {eta_str}", flush=True)
        
        # ç¨ä½œå»¶é²
        time.sleep(0.5)
    
    print()
    
    # æ¯æ‰¹å¾Œä¿å­˜ä¸€æ¬¡ï¼ˆä»¥é˜²ä¸­æ–·ï¼‰
    temp_file = SESSION_DIR / f"qwen_analysis_progress.json"
    with open(temp_file, 'w', encoding='utf-8') as f:
        json.dump({
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_processed": total_processed,
                "successful": successful,
                "failed": failed,
            },
            "results": analysis_results
        }, f, ensure_ascii=False, indent=2)

print("=" * 80)
print(f"âœ¨ åˆ†æå®Œæˆï¼š{datetime.now().strftime('%H:%M:%S')}")
print("=" * 80)
print(f"ç¸½è¨ˆï¼š{total_processed} å¼µåœ–ç‰‡")
print(f"æˆåŠŸï¼š{successful} å¼µ âœ…")
print(f"å¤±æ•—ï¼š{failed} å¼µ âŒ")
print()

# æ›´æ–°é€²åº¦ï¼šå®Œæˆåˆ†æ
progress.complete_analysis(successful, failed)

# ä¿å­˜å®Œæ•´åˆ†æçµæœ
with open(SESSION_DIR / "qwen_vision_analysis_complete.json", "w", encoding="utf-8") as f:
    json.dump({
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "total_analyzed": total_processed,
            "successful": successful,
            "failed": failed,
            "api_endpoint": LM_STUDIO_API,
            "model": "qwen/qwen3-vl-30b"
        },
        "detailed_results": analysis_results
    }, f, ensure_ascii=False, indent=2)

print(f"ğŸ’¾ å®Œæ•´åˆ†æçµæœå·²ä¿å­˜ï¼š")
print(f"   {SESSION_DIR / 'qwen_vision_analysis_complete.json'}")
print()

# ç”Ÿæˆå‘½åå°ç…§è¡¨å’Œé‡å‘½åè¨ˆç•«
print("ï¿½ï¿½ ç”Ÿæˆé‡å‘½åå°ç…§è¡¨...")

rename_plan = []
for result in analysis_results:
    if result['status'] == 'success':
        old_name = result['filename']
        analysis = result['analysis']
        new_name = analysis.get('recommended_name', 'UNKNOWN')
        
        # ç¢ºä¿æ–°åç¨±æœ‰å‰¯æª”å
        old_path = TARGET_DIR / old_name
        ext = old_path.suffix
        if not new_name.endswith(ext):
            new_name = new_name + ext
        
        rename_plan.append({
            "old_filename": old_name,
            "new_filename": new_name,
            "image_title": analysis.get('image_title', 'N/A'),
            "main_theme": analysis.get('main_theme', 'N/A'),
            "sub_theme": analysis.get('sub_theme', 'N/A'),
            "core_content": analysis.get('core_content', 'N/A')
        })

# æª¢æŸ¥é‡è¤‡çš„æ–°åç¨±
name_counts = {}
for item in rename_plan:
    new_name = item['new_filename']
    name_counts[new_name] = name_counts.get(new_name, 0) + 1

duplicates = {k: v for k, v in name_counts.items() if v > 1}
if duplicates:
    print(f"âš ï¸  è­¦å‘Šï¼šæª¢æ¸¬åˆ° {len(duplicates)} å€‹é‡è¤‡çš„æ–°åç¨±")
    # ç‚ºé‡è¤‡çš„åç¨±æ·»åŠ åºè™Ÿ
    new_name_count = {}
    for item in rename_plan:
        new_name = item['new_filename']
        if new_name in duplicates:
            new_name_count[new_name] = new_name_count.get(new_name, 0) + 1
            base, ext = new_name.rsplit('.', 1)
            item['new_filename'] = f"{base}_{new_name_count[new_name]:02d}.{ext}"

# ä¿å­˜å°ç…§è¡¨
with open(SESSION_DIR / "qwen_rename_plan_complete.json", "w", encoding="utf-8") as f:
    json.dump(rename_plan, f, ensure_ascii=False, indent=2)

print(f"âœ… å·²ç‚º {len(rename_plan)} å€‹æ–‡ä»¶ç”Ÿæˆæ–°åç¨±")
print(f"ğŸ“Š å°ç…§è¡¨å·²ä¿å­˜ï¼š{SESSION_DIR / 'qwen_rename_plan_complete.json'}")
print()

# åŸ·è¡Œé‡å‘½å
print("ğŸ”„ é–‹å§‹åŸ·è¡Œé‡å‘½å...")
print()

# æª¢æŸ¥æ˜¯å¦æœ‰éœ€è¦é‡å‘½åçš„æª”æ¡ˆ
if not rename_plan:
    print("[å®Œæˆ] â„¹ï¸ æ²’æœ‰æ‰¾åˆ°éœ€è¦é‡å‘½åçš„åœ–ç‰‡")
    print("[å®Œæˆ] âœ… æ‰€æœ‰æ“ä½œå·²å®Œæˆï¼", flush=True)
    print()
else:
    # æ›´æ–°é€²åº¦ï¼šé–‹å§‹é‡å‘½å
    progress.start_rename()

    renamed_count = 0
    deleted_count = 0
    rename_errors = []
    delete_errors = []

    for idx, item in enumerate(rename_plan, 1):
        old_path = TARGET_DIR / item['old_filename']
        new_path = TARGET_DIR / item['new_filename']
        
        try:
            if old_path.exists():
                if new_path.exists() and new_path != old_path:
                    # é¿å…è¦†è“‹ç¾æœ‰æª”æ¡ˆ
                    base, ext = new_path.name.rsplit('.', 1)
                    counter = 1
                    while new_path.exists():
                        new_path = TARGET_DIR / f"{base}_{counter:02d}.{ext}"
                        counter += 1
                    item['new_filename'] = new_path.name
                
                # å¦‚æœå‹¾é¸äº†ã€Œåˆªé™¤åŸæª”æ¡ˆã€ï¼Œå…ˆè¨˜éŒ„èˆŠæª”æ¡ˆè·¯å¾‘å’Œå…§å®¹
                should_delete_after_rename = DELETE_ORIGINAL
                
                # åŸ·è¡Œé‡å‘½åï¼ˆé€™æœƒå°‡ old_path æ›´åç‚º new_pathï¼‰
                old_path.rename(new_path)
                renamed_count += 1
                print(f"âœ… {item['old_filename'][:40]:<40} â†’ {new_path.name[:35]}")
                
                # âš ï¸ æ³¨æ„ï¼šrename() ä¹‹å¾Œï¼Œold_path ä¸å†å­˜åœ¨
                # æ‰€ä»¥ä¸éœ€è¦å†æ¬¡åˆªé™¤ old_path
                # å¦‚æœ should_delete_after_renameï¼Œé‚£éº¼åŸæª”æ¡ˆå·²ç¶“è¢«æ›¿æ›ç‚ºæ–°æª”æ¡ˆäº†
                # ä¸éœ€è¦é¡å¤–æ“ä½œ
                
                if should_delete_after_rename:
                    deleted_count += 1
                
                # è¨ˆç®—ä¸¦è¼¸å‡ºé€²åº¦ç™¾åˆ†æ¯”
                progress_pct = int(renamed_count * 100 / len(rename_plan)) if rename_plan else 0
                eta = progress.get_eta_seconds()
                eta_str = progress._format_time(eta) if eta > 0 else "è¨ˆç®—ä¸­..."
                print(f"[é€²åº¦] é‡å‘½å: {progress_pct}% | {renamed_count}/{len(rename_plan)} | ETA: {eta_str}", flush=True)
                
                # æ›´æ–°é€²åº¦
                progress.update_rename(idx)
        
        except Exception as e:
            rename_errors.append({
                "old": item['old_filename'],
                "new": item['new_filename'],
                "error": str(e)
            })
            print(f"âŒ {item['old_filename'][:40]:<40} (éŒ¯èª¤ï¼š{str(e)[:30]})")

print()
print("=" * 80)
print(f"âœ¨ é‡å‘½åå®Œæˆ")
print("=" * 80)

if rename_plan:
    # æ›´æ–°é€²åº¦ï¼šå®Œæˆé‡å‘½å
    progress.complete_rename(renamed_count, len(rename_errors))
    print(f"æˆåŠŸé‡å‘½åï¼š{renamed_count} å¼µ")
    print(f"é‡å‘½åå¤±æ•—ï¼š{len(rename_errors)} å¼µ")
    if DELETE_ORIGINAL:
        print(f"âœ… å·²åˆªé™¤åŸæª”æ¡ˆï¼ˆé‡å‘½åæ™‚è‡ªå‹•åˆªé™¤ï¼‰ï¼š{deleted_count} å¼µ")
else:
    renamed_count = 0
    deleted_count = 0
    rename_errors = []

print()

# è¼¸å‡ºæœ€çµ‚å®Œæˆè¨Šæ¯ï¼ˆç¢ºä¿ GUI èƒ½çœ‹åˆ°ï¼‰
print("[å®Œæˆ] âœ… æ‰€æœ‰æ“ä½œå·²å®Œæˆï¼", flush=True)
print(f"[å®Œæˆ] ğŸ“Š çµ±è¨ˆï¼šå…±è™•ç† {total_processed} å¼µåœ–ç‰‡", flush=True)
print(f"[å®Œæˆ] â±ï¸  ç¸½è€—æ™‚ï¼š{progress._format_time(time.time() - progress.start_time)}", flush=True)
print()

# ä¿å­˜æœ€çµ‚å ±å‘Š
final_report = {
    "timestamp": datetime.now().isoformat(),
    "total_images": len(image_files),
    "analyzed": total_processed,
    "successful_analysis": successful,
    "failed_analysis": failed,
    "renamed": renamed_count,
    "rename_errors": len(rename_errors),
    "deleted": deleted_count if DELETE_ORIGINAL else 0,
    "errors": rename_errors if rename_errors else []
}

with open(SESSION_DIR / "qwen_rename_final_report.json", "w", encoding="utf-8") as f:
    json.dump(final_report, f, ensure_ascii=False, indent=2)

print(f"ğŸ“ æœ€çµ‚å ±å‘Šå·²ä¿å­˜ï¼š{SESSION_DIR / 'qwen_rename_final_report.json'}")

