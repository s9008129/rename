#!/usr/bin/env python3
"""
å®Œæ•´åŸ·è¡Œï¼šä½¿ç”¨ Qwen3-VL é€²è¡Œå…¨é‡åœ–ç‰‡åˆ†æå’Œç²¾æº–é‡å‘½å

æµç¨‹ï¼š
1. åˆ†æå…¨éƒ¨åœ–ç‰‡
2. ç”Ÿæˆç²¾æº–å‘½åå°ç…§è¡¨
3. åŸ·è¡Œæª”æ¡ˆé‡å‘½å
4. ç”Ÿæˆè©³ç´°å ±å‘Š

æ–°å¢åŠŸèƒ½ï¼ˆv1.1ï¼‰ï¼š
- å¢é‡æ¨¡å¼ï¼ˆé»˜èªï¼‰ï¼šè·³éå·²å‘½åçš„æª”æ¡ˆï¼ˆæª”ååŒ…å«ä¸­æ–‡ï¼‰
- å¼·åˆ¶é‡æ–°å‘½åæ¨¡å¼ï¼šé‡æ–°åˆ†æå’Œå‘½åæ‰€æœ‰æª”æ¡ˆ
- å…¨å±€æª”æ¡ˆè¿½è¹¤æ©Ÿåˆ¶
"""

import os
import json
import base64
import requests
from pathlib import Path
from typing import Dict, List, Optional
import time
from datetime import datetime
import argparse
import sys

# é…ç½®
# ä½¿ç”¨ç›¸å°è·¯å¾‘ï¼šPROJECT_ROOT æ‡‰è©²æ˜¯åŸ·è¡Œè…³æœ¬çš„ç›®éŒ„
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
LOGS_DIR = PROJECT_ROOT / "logs"

LM_STUDIO_API = "http://127.0.0.1:1234/v1/chat/completions"
BATCH_SIZE = 10  # æ¯æ‰¹ 10 å¼µåœ–ç‰‡

# ç¢ºä¿å¿…è¦çš„ç›®éŒ„å­˜åœ¨
DATA_DIR.mkdir(exist_ok=True)
LOGS_DIR.mkdir(exist_ok=True)

# è§£æå‘½ä»¤è¡Œåƒæ•¸
parser = argparse.ArgumentParser(
    description="åœ–ç‰‡æ™ºèƒ½å‘½åç³»çµ± - ä½¿ç”¨ Qwen3-VL é€²è¡Œè¦–è¦ºåˆ†æå’Œé‡å‘½å"
)
parser.add_argument(
    "--force-rename",
    "--override",
    dest="force_rename",
    action="store_true",
    help="å¼·åˆ¶é‡æ–°å‘½åå·²å‘½åçš„æª”æ¡ˆï¼ˆå¢é‡æ¨¡å¼ï¼‰"
)
parser.add_argument(
    "--target-dir",
    default=None,
    help="æŒ‡å®šè¦è™•ç†çš„ç›®éŒ„ï¼ˆé»˜èªï¼šä½¿ç”¨äº¤äº’å¼æç¤ºè¼¸å…¥ï¼‰"
)
args = parser.parse_args()

FORCE_RENAME = args.force_rename

# å¦‚æœæ²’æœ‰æŒ‡å®šç›®éŒ„ï¼Œä½¿ç”¨äº¤äº’å¼è¼¸å…¥æˆ–ç•¶å‰ç›®éŒ„
if args.target_dir:
    TARGET_DIR = Path(args.target_dir).expanduser()
else:
    # é»˜èªç‚ºç•¶å‰å·¥ä½œç›®éŒ„
    TARGET_DIR = Path.cwd()

print("=" * 80)
print("ğŸš€ åœ–ç‰‡æ™ºèƒ½å‘½åç³»çµ± - Qwen3-VL æ‰¹é‡åˆ†æå’Œé‡å‘½å")
print("=" * 80)
print(f"æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ç›®æ¨™ç›®éŒ„ï¼š{TARGET_DIR}")
if FORCE_RENAME:
    print("ğŸ“Œ æ¨¡å¼ï¼šå¼·åˆ¶é‡æ–°å‘½åï¼ˆå°‡é‡æ–°åˆ†ææ‰€æœ‰æª”æ¡ˆï¼‰")
else:
    print("ğŸ“Œ æ¨¡å¼ï¼šå¢é‡æ¨¡å¼ï¼ˆå°‡è·³éå·²å‘½åçš„æª”æ¡ˆï¼‰")
print()

def is_already_renamed(filename: str) -> bool:
    """æª¢æ¸¬æª”æ¡ˆæ˜¯å¦å·²è¢«å‘½åï¼ˆæª”ååŒ…å«ä¸­æ–‡å­—ç¬¦ï¼‰"""
    import re
    return bool(re.search(r'[\u4e00-\u9fff]', filename))

# æƒææ‰€æœ‰åœ–ç‰‡
image_files = sorted([
    f for f in TARGET_DIR.glob("*") 
    if f.is_file() and f.suffix.lower() in {'.png', '.jpg', '.jpeg', '.webp', '.gif'}
])

print(f"ğŸ“Š æƒæçµæœï¼šæ‰¾åˆ° {len(image_files)} å€‹åœ–ç‰‡æª”æ¡ˆ")

# æª¢æ¸¬å·²å‘½åå’Œæœªå‘½åçš„æª”æ¡ˆ
if not FORCE_RENAME:
    renamed_files = [f for f in image_files if is_already_renamed(f.stem)]
    unnamed_files = [f for f in image_files if not is_already_renamed(f.stem)]
    
    print(f"   å·²å‘½åï¼š{len(renamed_files)} å€‹")
    print(f"   æœªå‘½åï¼š{len(unnamed_files)} å€‹")
    
    if renamed_files:
        print(f"   ğŸ’¡ æç¤ºï¼šå·²å‘½åçš„æª”æ¡ˆå°‡è¢«è·³éã€‚ä½¿ç”¨ --force-rename é‡æ–°åˆ†ææ‰€æœ‰æª”æ¡ˆ")
    
    # å¢é‡æ¨¡å¼ï¼šåªè™•ç†æœªå‘½åçš„æª”æ¡ˆ
    image_files = unnamed_files
    print()
    print(f"âš™ï¸  é–‹å§‹è™•ç† {len(image_files)} å€‹æœªå‘½åçš„æª”æ¡ˆ...")
else:
    print(f"   æ‰¹æ¬¡å¤§å°ï¼š{BATCH_SIZE} å¼µ/æ‰¹")
    print(f"   é è¨ˆæ‰¹æ¬¡æ•¸ï¼š{(len(image_files) + BATCH_SIZE - 1) // BATCH_SIZE}")
    print()

print()

# åˆ†æçµæœå„²å­˜
analysis_results = []
failed_files = []
skipped_duplicates = []

def encode_image_to_base64(image_path: Path) -> str:
    """å°‡åœ–ç‰‡ç·¨ç¢¼ç‚º base64"""
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

def get_image_media_type(image_path: Path) -> str:
    """æ ¹æ“šå‰¯æª”åç¢ºå®š MIME é¡å‹"""
    ext = image_path.suffix.lower()
    return {
        '.png': 'image/png',
        '.jpg': 'image/jpeg',
        '.jpeg': 'image/jpeg',
        '.webp': 'image/webp',
        '.gif': 'image/gif'
    }.get(ext, 'image/png')

def analyze_image_with_qwen(image_path: Path, retry_count: int = 3) -> Dict:
    """ä½¿ç”¨ Qwen3-VL åˆ†æå–®å¼µåœ–ç‰‡ï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰"""
    
    for attempt in range(retry_count):
        try:
            # ç·¨ç¢¼åœ–ç‰‡
            image_base64 = encode_image_to_base64(image_path)
            media_type = get_image_media_type(image_path)
            
            # æº–å‚™åˆ†ææç¤º
            analysis_prompt = """è«‹æ·±åº¦åˆ†æé€™å¼µåœ–ç‰‡ä¸¦ç”¨å°ç£ç¹é«”ä¸­æ–‡å›ç­”ã€‚è¿”å› JSON æ ¼å¼çš„çµæœï¼ˆåªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š

{
  "image_title": "åœ–ç‰‡ä¸­çš„æ¨™é¡Œæ–‡å­—ï¼ˆå¦‚ç„¡æ¨™é¡Œå‰‡ç‚º 'N/A'ï¼‰",
  "main_theme": "æ ¸å¿ƒä¸»é¡Œåˆ†é¡ï¼ˆå¦‚ï¼šè²¡ç¶“ã€æŠ€è¡“ã€è¨­è¨ˆã€å ±å‘Šç­‰ï¼‰",
  "sub_theme": "å­åˆ†é¡ï¼ˆå¦‚ï¼šæŠ•è³‡åˆ†æã€AIç³»çµ±ã€å‰µæ„è¨­è¨ˆç­‰ï¼‰",
  "core_content": "åœ–ç‰‡çš„å…·é«”æ ¸å¿ƒå…§å®¹ï¼ˆé—œéµè©æˆ–çŸ­å¥ï¼Œ20å­—ä»¥å…§ï¼‰",
  "recommended_name": "æ¨è–¦å‘½åï¼ˆæ ¼å¼ï¼šä¸»é¡Œ_å­ä¸»é¡Œ_å…·é«”æ¨™é¡Œï¼Œæœ€å¤š25å­—ï¼Œä¸å«æ—¥æœŸï¼‰"
}"""
            
            # èª¿ç”¨ LM Studio API
            headers = {"Content-Type": "application/json"}
            
            payload = {
                "model": "qwen/qwen3-vl-30b",
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{media_type};base64,{image_base64}"
                                }
                            },
                            {
                                "type": "text",
                                "text": analysis_prompt
                            }
                        ]
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 500
            }
            
            response = requests.post(LM_STUDIO_API, json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            
            # è§£æå›æ‡‰
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            
            # æå– JSON
            try:
                analysis_json = json.loads(analysis_text)
            except json.JSONDecodeError:
                import re
                json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
                if json_match:
                    analysis_json = json.loads(json_match.group())
                else:
                    raise ValueError(f"ç„¡æ³•è§£æå›æ‡‰")
            
            return {
                "filename": image_path.name,
                "status": "success",
                "analysis": analysis_json
            }
        
        except Exception as e:
            if attempt < retry_count - 1:
                time.sleep(2)  # é‡è©¦å‰ç­‰å¾…
                continue
            else:
                return {
                    "filename": image_path.name,
                    "status": "error",
                    "error": str(e)
                }

# åŠ è¼‰ä¹‹å‰çš„çµæœï¼ˆå¦‚æœæœ‰ï¼‰
previous_results_file = SESSION_DIR / "qwen_vision_analysis_sample.json"
if previous_results_file.exists():
    print("ğŸ“‚ åŠ è¼‰ä¹‹å‰çš„æ¨£æœ¬åˆ†æçµæœ...")
    with open(previous_results_file, 'r', encoding='utf-8') as f:
        previous = json.load(f)
        analysis_results = previous.get('detailed_results', [])
    print(f"   å·²åŠ è¼‰ {len(analysis_results)} å€‹çµæœ")
    processed_files = {r['filename'] for r in analysis_results}
    remaining_files = [f for f in image_files if f.name not in processed_files]
    print(f"   å‰©é¤˜å¾…åˆ†æï¼š{len(remaining_files)} å¼µ")
    print()
else:
    remaining_files = image_files
    processed_files = set()

# æ‰¹é‡è™•ç†åœ–ç‰‡
print("ğŸš€ é–‹å§‹å…¨é‡åˆ†æ...")
print()

total_processed = len(analysis_results)
successful = sum(1 for r in analysis_results if r['status'] == 'success')
failed = sum(1 for r in analysis_results if r['status'] != 'success')

for batch_idx in range((len(remaining_files) + BATCH_SIZE - 1) // BATCH_SIZE):
    start_idx = batch_idx * BATCH_SIZE
    end_idx = min(start_idx + BATCH_SIZE, len(remaining_files))
    
    batch_files = remaining_files[start_idx:end_idx]
    batch_num = len(analysis_results) // BATCH_SIZE + batch_idx + 1
    
    print(f"ğŸ“¦ Batch {batch_num} ({len(batch_files)} å¼µ | é€²åº¦ï¼š{total_processed + len(batch_files)}/{len(image_files)})")
    
    for img_idx, img_file in enumerate(batch_files, 1):
        print(f"   [{img_idx}/{len(batch_files)}] {img_file.name[:45]}... ", end="", flush=True)
        
        result = analyze_image_with_qwen(img_file)
        analysis_results.append(result)
        total_processed += 1
        
        if result['status'] == 'success':
            successful += 1
            print(f"âœ…")
        else:
            failed += 1
            failed_files.append(result)
            print(f"âŒ")
        
        # ç¨ä½œå»¶é²
        time.sleep(0.5)
    
    print()
    
    # æ¯æ‰¹å¾Œä¿å­˜ä¸€æ¬¡ï¼ˆä»¥é˜²ä¸­æ–·ï¼‰
    temp_file = SESSION_DIR / f"qwen_analysis_progress.json"
    with open(temp_file, 'w', encoding='utf-8') as f:
        json.dump({
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_processed": total_processed,
                "successful": successful,
                "failed": failed,
            },
            "results": analysis_results
        }, f, ensure_ascii=False, indent=2)

print("=" * 80)
print(f"âœ¨ åˆ†æå®Œæˆï¼š{datetime.now().strftime('%H:%M:%S')}")
print("=" * 80)
print(f"ç¸½è¨ˆï¼š{total_processed} å¼µåœ–ç‰‡")
print(f"æˆåŠŸï¼š{successful} å¼µ âœ…")
print(f"å¤±æ•—ï¼š{failed} å¼µ âŒ")
print()

# ä¿å­˜å®Œæ•´åˆ†æçµæœ
with open(SESSION_DIR / "qwen_vision_analysis_complete.json", "w", encoding="utf-8") as f:
    json.dump({
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "total_analyzed": total_processed,
            "successful": successful,
            "failed": failed,
            "api_endpoint": LM_STUDIO_API,
            "model": "qwen/qwen3-vl-30b"
        },
        "detailed_results": analysis_results
    }, f, ensure_ascii=False, indent=2)

print(f"ğŸ’¾ å®Œæ•´åˆ†æçµæœå·²ä¿å­˜ï¼š")
print(f"   {SESSION_DIR / 'qwen_vision_analysis_complete.json'}")
print()

# ç”Ÿæˆå‘½åå°ç…§è¡¨å’Œé‡å‘½åè¨ˆç•«
print("ï¿½ï¿½ ç”Ÿæˆé‡å‘½åå°ç…§è¡¨...")

rename_plan = []
for result in analysis_results:
    if result['status'] == 'success':
        old_name = result['filename']
        analysis = result['analysis']
        new_name = analysis.get('recommended_name', 'UNKNOWN')
        
        # ç¢ºä¿æ–°åç¨±æœ‰å‰¯æª”å
        old_path = DOWNLOADS_DIR / old_name
        ext = old_path.suffix
        if not new_name.endswith(ext):
            new_name = new_name + ext
        
        rename_plan.append({
            "old_filename": old_name,
            "new_filename": new_name,
            "image_title": analysis.get('image_title', 'N/A'),
            "main_theme": analysis.get('main_theme', 'N/A'),
            "sub_theme": analysis.get('sub_theme', 'N/A'),
            "core_content": analysis.get('core_content', 'N/A')
        })

# æª¢æŸ¥é‡è¤‡çš„æ–°åç¨±
name_counts = {}
for item in rename_plan:
    new_name = item['new_filename']
    name_counts[new_name] = name_counts.get(new_name, 0) + 1

duplicates = {k: v for k, v in name_counts.items() if v > 1}
if duplicates:
    print(f"âš ï¸  è­¦å‘Šï¼šæª¢æ¸¬åˆ° {len(duplicates)} å€‹é‡è¤‡çš„æ–°åç¨±")
    # ç‚ºé‡è¤‡çš„åç¨±æ·»åŠ åºè™Ÿ
    new_name_count = {}
    for item in rename_plan:
        new_name = item['new_filename']
        if new_name in duplicates:
            new_name_count[new_name] = new_name_count.get(new_name, 0) + 1
            base, ext = new_name.rsplit('.', 1)
            item['new_filename'] = f"{base}_{new_name_count[new_name]:02d}.{ext}"

# ä¿å­˜å°ç…§è¡¨
with open(SESSION_DIR / "qwen_rename_plan_complete.json", "w", encoding="utf-8") as f:
    json.dump(rename_plan, f, ensure_ascii=False, indent=2)

print(f"âœ… å·²ç‚º {len(rename_plan)} å€‹æ–‡ä»¶ç”Ÿæˆæ–°åç¨±")
print(f"ğŸ“Š å°ç…§è¡¨å·²ä¿å­˜ï¼š{SESSION_DIR / 'qwen_rename_plan_complete.json'}")
print()

# åŸ·è¡Œé‡å‘½å
print("ğŸ”„ é–‹å§‹åŸ·è¡Œé‡å‘½å...")
print()

renamed_count = 0
rename_errors = []

for item in rename_plan:
    old_path = DOWNLOADS_DIR / item['old_filename']
    new_path = DOWNLOADS_DIR / item['new_filename']
    
    try:
        if old_path.exists():
            if new_path.exists() and new_path != old_path:
                # é¿å…è¦†è“‹ç¾æœ‰æª”æ¡ˆ
                base, ext = new_path.name.rsplit('.', 1)
                counter = 1
                while new_path.exists():
                    new_path = DOWNLOADS_DIR / f"{base}_{counter:02d}.{ext}"
                    counter += 1
                item['new_filename'] = new_path.name
            
            old_path.rename(new_path)
            renamed_count += 1
            print(f"âœ… {item['old_filename'][:40]:<40} â†’ {new_path.name[:35]}")
    
    except Exception as e:
        rename_errors.append({
            "old": item['old_filename'],
            "new": item['new_filename'],
            "error": str(e)
        })
        print(f"âŒ {item['old_filename'][:40]:<40} (éŒ¯èª¤ï¼š{str(e)[:30]})")

print()
print("=" * 80)
print(f"âœ¨ é‡å‘½åå®Œæˆ")
print("=" * 80)
print(f"æˆåŠŸé‡å‘½åï¼š{renamed_count} å¼µ")
print(f"é‡å‘½åå¤±æ•—ï¼š{len(rename_errors)} å¼µ")
print()

# ä¿å­˜æœ€çµ‚å ±å‘Š
final_report = {
    "timestamp": datetime.now().isoformat(),
    "total_images": len(image_files),
    "analyzed": total_processed,
    "successful_analysis": successful,
    "failed_analysis": failed,
    "renamed": renamed_count,
    "rename_errors": len(rename_errors),
    "errors": rename_errors if rename_errors else []
}

with open(SESSION_DIR / "qwen_rename_final_report.json", "w", encoding="utf-8") as f:
    json.dump(final_report, f, ensure_ascii=False, indent=2)

print(f"ğŸ“ æœ€çµ‚å ±å‘Šå·²ä¿å­˜ï¼š{SESSION_DIR / 'qwen_rename_final_report.json'}")

